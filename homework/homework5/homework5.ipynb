{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdc3acc-253f-490f-8ca9-62fd70718586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skeleton ready:\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/raw\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/processed\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/.env\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/README.md\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/.gitignore\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "# 假设本 notebook 就在 homework/homework05/ 下\n",
    "HW_DIR = Path.cwd()\n",
    "assert HW_DIR.name.lower().startswith(\"homework5\"), f\"请在 homework5 目录下运行，当前：{HW_DIR}\"\n",
    "\n",
    "# 标准结构\n",
    "DATA_RAW = HW_DIR / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = HW_DIR / \"data\" / \"processed\"\n",
    "SRC_DIR = HW_DIR / \"src\"\n",
    "STORAGE_DIR = SRC_DIR / \"storage\"\n",
    "UTILS_DIR = SRC_DIR / \"utils\"\n",
    "\n",
    "# 创建目录（幂等）\n",
    "for p in [DATA_RAW, DATA_PROCESSED, STORAGE_DIR, UTILS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 写入 .env（若不存在则新建；存在则保留你的旧设置）\n",
    "env_path = HW_DIR / \".env\"\n",
    "if not env_path.exists():\n",
    "    env_path.write_text(\"DATA_DIR_RAW=data/raw\\nDATA_DIR_PROCESSED=data/processed\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# 写入 README.md（若不存在则生成基础模板）\n",
    "readme_path = HW_DIR / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_md = dedent(\"\"\"\n",
    "    # Homework 05 — Data Storage\n",
    "\n",
    "    ## Folder Structure\n",
    "    - `data/raw/` – first-touch CSV\n",
    "    - `data/processed/` – Parquet (columnar, compressed)\n",
    "    - `notebooks/` – this notebook\n",
    "    - `src/` – utilities for I/O and validation\n",
    "\n",
    "    ## Environment\n",
    "    A local `.env` in *this* folder controls where data is written:\n",
    "    ```\n",
    "    DATA_DIR_RAW=data/raw\n",
    "    DATA_DIR_PROCESSED=data/processed\n",
    "    ```\n",
    "\n",
    "    ## How to Run\n",
    "    Execute cells in the notebook. It will:\n",
    "    1) Create folders and `.env`\n",
    "    2) Save the sample DataFrame to CSV & Parquet\n",
    "    3) Reload both and validate shapes/dtypes\n",
    "    4) Use suffix-based utilities `write_df` / `read_df`\n",
    "    \"\"\").strip() + \"\\n\"\n",
    "    readme_path.write_text(readme_md, encoding=\"utf-8\")\n",
    "\n",
    "# 可选：作业内独立 .gitignore（防止误提交数据）\n",
    "gitignore_path = HW_DIR / \".gitignore\"\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(dedent(\"\"\"\n",
    "    .ipynb_checkpoints/\n",
    "    .env\n",
    "    data/raw/**\n",
    "    data/processed/**\n",
    "    \"\"\").strip()+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Skeleton ready:\")\n",
    "print(f\"- {DATA_RAW}\")\n",
    "print(f\"- {DATA_PROCESSED}\")\n",
    "print(f\"- {env_path}\")\n",
    "print(f\"- {readme_path}\")\n",
    "print(f\"- {gitignore_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ef074b-b989-4db1-a001-d391ea11c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Utilities written to:\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/src/storage/io_utils.py\n",
      "- /Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/src/utils/validate.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "io_utils = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_parent(path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: Path, index: bool = False) -> None:\n",
    "    \\\"\\\"\\\"Write by suffix (.csv / .parquet). Creates parent dirs. Clear error if parquet engine missing.\\\"\\\"\\\"\n",
    "    _ensure_parent(path)\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".csv\":\n",
    "        df.to_csv(path, index=index)\n",
    "    elif suf == \".parquet\":\n",
    "        try:\n",
    "            df.to_parquet(path, index=index)  # auto-detect engine; prefers pyarrow\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to write Parquet. Install a Parquet engine, e.g. `pip install pyarrow`\\\\n\"\n",
    "                f\"Original error: {e}\"\n",
    "            ) from e\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported suffix: {suf}. Use .csv or .parquet.\")\n",
    "\n",
    "def read_df(path: Path, parse_dates: Optional[Sequence[str]] = None) -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"Read by suffix (.csv / .parquet).\\\"\\\"\\\"\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".csv\":\n",
    "        return pd.read_csv(path, parse_dates=list(parse_dates) if parse_dates else None)\n",
    "    elif suf == \".parquet\":\n",
    "        try:\n",
    "            return pd.read_parquet(path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to read Parquet. Install `pyarrow`.\\\\n\"\n",
    "                f\"Original error: {e}\"\n",
    "            ) from e\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported suffix: {suf}. Use .csv or .parquet.\")\n",
    "\"\"\").strip()+\"\\n\"\n",
    "\n",
    "validate_utils = dedent(\"\"\"\n",
    "from typing import Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "def validate_df(original: pd.DataFrame, reloaded: pd.DataFrame, expected_dtypes: Optional[Dict[str, str]] = None):\n",
    "    \\\"\\\"\\\"Return dict: check_name -> (ok: bool, message: str).\\\"\\\"\\\"\n",
    "    results = {}\n",
    "    # shape\n",
    "    same_shape = original.shape == reloaded.shape\n",
    "    results[\"shape_match\"] = (same_shape, f\"original={original.shape}, reloaded={reloaded.shape}\")\n",
    "    # columns (order-sensitive for this HW)\n",
    "    same_cols = list(original.columns) == list(reloaded.columns)\n",
    "    results[\"columns_match_order\"] = (same_cols, f\"original={list(original.columns)}, reloaded={list(reloaded.columns)}\")\n",
    "    # dtype checks (allow CSV to degrade 'category' -> 'object'/'string')\n",
    "    if expected_dtypes:\n",
    "        dtype_ok = True\n",
    "        msgs = []\n",
    "        for col, exp in expected_dtypes.items():\n",
    "            if col not in reloaded.columns:\n",
    "                dtype_ok = False\n",
    "                msgs.append(f\"{col}: MISSING\")\n",
    "                continue\n",
    "            got = str(reloaded[col].dtype)\n",
    "            if exp == \"category\" and got in (\"category\",\"object\",\"string\"):\n",
    "                msgs.append(f\"{col}: OK (got {got}, expected {exp} acceptable)\")\n",
    "            elif got != exp:\n",
    "                dtype_ok = False\n",
    "                msgs.append(f\"{col}: got {got}, expected {exp}\")\n",
    "            else:\n",
    "                msgs.append(f\"{col}: OK ({got})\")\n",
    "        results[\"dtypes_expected\"] = (dtype_ok, \"; \".join(msgs))\n",
    "    return results\n",
    "\"\"\").strip()+\"\\n\"\n",
    "\n",
    "(STORAGE_DIR / \"io_utils.py\").write_text(io_utils, encoding=\"utf-8\")\n",
    "(UTILS_DIR / \"validate.py\").write_text(validate_utils, encoding=\"utf-8\")\n",
    "\n",
    "# 让 src 变成包（空 __init__.py）\n",
    "for d in [SRC_DIR, STORAGE_DIR, UTILS_DIR]:\n",
    "    init_p = d / \"__init__.py\"\n",
    "    if not init_p.exists():\n",
    "        init_p.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Utilities written to:\")\n",
    "print(\"-\", STORAGE_DIR / \"io_utils.py\")\n",
    "print(\"-\", UTILS_DIR / \"validate.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6834bd1f-18e1-4981-875d-09a3d88a17fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/raw'),\n",
       " PosixPath('/Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/processed'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载作业内 .env\n",
    "load_dotenv(HW_DIR / \".env\")\n",
    "\n",
    "DATA_DIR_RAW = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "DATA_DIR_PROCESSED = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "\n",
    "# 相对作业目录\n",
    "DATA_DIR_RAW = (HW_DIR / DATA_DIR_RAW).resolve()\n",
    "DATA_DIR_PROCESSED = (HW_DIR / DATA_DIR_PROCESSED).resolve()\n",
    "\n",
    "DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR_RAW, DATA_DIR_PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c77aed-9a67-4108-b3e6-9a852ce26143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(date      datetime64[ns]\n",
       " ticker          category\n",
       " close            float64\n",
       " volume             int64\n",
       " dtype: object,\n",
       "         date ticker   close   volume\n",
       " 0 2024-01-01    SPY  450.00  1089250\n",
       " 1 2024-01-02    SPY  452.22  1773956\n",
       " 2 2024-01-03    SPY  454.44  1654571\n",
       " 3 2024-01-04    SPY  456.67  1438878\n",
       " 4 2024-01-05    SPY  458.89  1433015)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "dates = pd.date_range(\"2024-01-01\", periods=10, freq=\"D\")\n",
    "df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"ticker\": pd.Series([\"SPY\"]*10, dtype=\"category\"),\n",
    "    \"close\": np.linspace(450.0, 470.0, 10).round(2),\n",
    "    \"volume\": rng.integers(1_000_000, 2_000_000, size=10, dtype=np.int64)\n",
    "})\n",
    "df.dtypes, df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a69355-60de-4147-8ee6-d89de324b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Parquet not saved: Failed to write Parquet. Install a Parquet engine, e.g. `pip install pyarrow`\n",
      "Original error: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/raw/sample_prices.csv'),\n",
       " PosixPath('/Users/mengmeng/bootcamp_Shuchen_Meng/homework/homework5/data/processed/sample_prices.parquet'),\n",
       " False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.storage.io_utils import write_df\n",
    "csv_path = DATA_DIR_RAW / \"sample_prices.csv\"\n",
    "parq_path = DATA_DIR_PROCESSED / \"sample_prices.parquet\"\n",
    "\n",
    "# 写 CSV\n",
    "write_df(df, csv_path, index=False)\n",
    "\n",
    "# 写 Parquet（若没装 pyarrow，会抛出带安装提示的错误；此作业允许）\n",
    "parquet_saved = True\n",
    "try:\n",
    "    write_df(df, parq_path, index=False)\n",
    "except Exception as e:\n",
    "    parquet_saved = False\n",
    "    print(\"⚠️ Parquet not saved:\", e)\n",
    "\n",
    "csv_path, parq_path, parquet_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9adf0b-a797-4131-b33d-e6d675f1ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV validation:\n",
      " - shape_match: ✅  original=(10, 4), reloaded=(10, 4)\n",
      " - columns_match_order: ✅  original=['date', 'ticker', 'close', 'volume'], reloaded=['date', 'ticker', 'close', 'volume']\n",
      " - dtypes_expected: ✅  date: OK (datetime64[ns]); ticker: OK (got object, expected category acceptable); close: OK (float64); volume: OK (int64)\n",
      "\n",
      "Parquet validation: skipped (not saved)\n"
     ]
    }
   ],
   "source": [
    "from src.storage.io_utils import read_df\n",
    "from src.utils.validate import validate_df\n",
    "\n",
    "# 读 CSV（解析日期）\n",
    "df_csv = read_df(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# 读 Parquet（如果写入失败则跳过）\n",
    "df_parq = read_df(parq_path) if parquet_saved else None\n",
    "\n",
    "expected = {\n",
    "    \"date\": \"datetime64[ns]\",\n",
    "    \"ticker\": \"category\",   # CSV 读回可能是 object/string，我们在校验里接受\n",
    "    \"close\": \"float64\",\n",
    "    \"volume\": \"int64\",\n",
    "}\n",
    "\n",
    "print(\"CSV validation:\")\n",
    "for k, (ok, msg) in validate_df(df, df_csv, expected).items():\n",
    "    print(f\" - {k}: {'✅' if ok else '❌'}  {msg}\")\n",
    "\n",
    "if df_parq is not None:\n",
    "    print(\"\\nParquet validation:\")\n",
    "    for k, (ok, msg) in validate_df(df, df_parq, expected).items():\n",
    "        print(f\" - {k}: {'✅' if ok else '❌'}  {msg}\")\n",
    "else:\n",
    "    print(\"\\nParquet validation: skipped (not saved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a975189-83e7-4321-a1bf-ce4756ad5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_md = f\"\"\"\n",
    "## Data Storage (Auto-Generated)\n",
    "\n",
    "**Folders**\n",
    "- `{DATA_DIR_RAW.relative_to(HW_DIR)}/` – first-touch CSV\n",
    "- `{DATA_DIR_PROCESSED.relative_to(HW_DIR)}/` – Parquet for analytics\n",
    "\n",
    "**Formats & Why**\n",
    "- **CSV**: simple, universal; larger on disk, slower for analytics.\n",
    "- **Parquet**: columnar + compressed (via `pyarrow`); smaller & faster for analytics.  \n",
    "  If `pyarrow` is missing, code shows a clear install hint.\n",
    "\n",
    "**Env-Driven Paths**\n",
    "Values come from `.env` in this folder:\n",
    "```\n",
    "DATA_DIR_RAW=data/raw\n",
    "DATA_DIR_PROCESSED=data/processed\n",
    "\\`\\`\\`\n",
    "\n",
    "**Utilities**\n",
    "Suffix-routed I/O:\n",
    "\\`\\`\\`python\n",
    "from src.storage.io_utils import write_df, read_df\n",
    "\n",
    "write_df(df, DATA_DIR_RAW / \"table.csv\")\n",
    "write_df(df, DATA_DIR_PROCESSED / \"table.parquet\")\n",
    "\n",
    "df_csv = read_df(DATA_DIR_RAW / \"table.csv\", parse_dates=[\"date\"])\n",
    "df_parq = read_df(DATA_DIR_PROCESSED / \"table.parquet\")\n",
    "\\`\\`\\`\n",
    "\n",
    "**Validation**\n",
    "The notebook prints checks for shape/columns/dtypes using `validate_df`.\n",
    "\"\"\"\n",
    "\n",
    "with open(HW_DIR / \"README.md\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\" + append_md.strip() + \"\\n\")\n",
    "\n",
    "print(\"✅ Data Storage section appended to homework05/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9d3d1-272d-44f7-9ab9-4cada719b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**Utilities**\n",
    "Suffix-routed I/O:\n",
    "```python\n",
    "from src.storage.io_utils import write_df, read_df\n",
    "\n",
    "write_df(df, Path(\"{(DATA_DIR_RAW/ 'sample_prices.csv').relative_to(HW_DIR)}\"))\n",
    "write_df(df, Path(\"{(DATA_DIR_PROCESSED/ 'sample_prices.parquet').relative_to(HW_DIR)}\"))\n",
    "\n",
    "df_csv = read_df(Path(\"{(DATA_DIR_RAW/ 'sample_prices.csv').relative_to(HW_DIR)}\"), parse_dates=[\"date\"])\n",
    "df_parq = read_df(Path(\"{(DATA_DIR_PROCESSED/ 'sample_prices.parquet').relative_to(HW_DIR)}\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bootcamp_env)",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
